{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24831ce9-b44f-4889-8cab-d74fc6d2a132",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 十一、小参数量微调\n",
    "\n",
    "<img src=\"peft_model.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "- 定义微调数据集加载器\n",
    "- 定义数据处理函数\n",
    "- 加载预训练模型：AutoModel.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "- 在预训练模型上增加任务相关输出层 （如果需要）\n",
    "- 加载预训练 Tokenizer：AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "- **定义注入参数的方法（见下文）**\n",
    "- 定义各种超参\n",
    "- 定义 Trainer\n",
    "- 定义 Evaluation Metric\n",
    "- 开始训练\n",
    "\n",
    "### 11.1、Prompt Tuning\n",
    "\n",
    "- 在输入序列前，额外加入一组伪 Embedding 向量\n",
    "- 只训练这组伪 Embedding，从而达到参数微调的效果\n",
    "\n",
    "<img src=\"soft-prompt.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 11.2、P-Tuning\n",
    "\n",
    "- 用一个生成器生成上述伪 Embedding\n",
    "- 只有生成器的参数是可训练的\n",
    "\n",
    "<img src=\"pt.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 11.3、Prefix-Tuning\n",
    "\n",
    "- 伪造前面的 Hidden States\n",
    "- 只训练伪造的这个 Prefix\n",
    "\n",
    "<img src=\"pt2.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 11.4、LoRA\n",
    "\n",
    "- 在 Transformer 的参数矩阵上加一个低秩矩阵（$A\\times B$）\n",
    "- 只训练 A，B\n",
    "- 理论上可以把上述方法应用于 Transformer 中的任意参数矩阵，包括 Embedding 矩阵\n",
    "- 通常应用于 Query, Value 两个参数矩阵\n",
    "\n",
    "<img src=\"lora.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 11.5、QLoRA\n",
    "\n",
    "什么是模型量化\n",
    "\n",
    "<img src=\"float.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "<img src=\"quant.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "更多参考: https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "\n",
    "QLoRA 引入了许多创新来在不牺牲性能的情况下节省显存：\n",
    "\n",
    "- 4位 NormalFloat（NF4），一种对于正态分布权重而言信息理论上最优的新数据类型\n",
    "- 双重量化，通过量化量化常数来减少平均内存占用\n",
    "- 分页优化器，用于管理内存峰值\n",
    "\n",
    "原文实现：单个48G的GPU显卡上微调65B的参数模型，保持16字节微调任务的性能\n",
    "\n",
    "### 11.6、AdaLoRA\n",
    "\n",
    "- 不预先指定可训练矩阵的秩\n",
    "- 根据参数矩阵的重要性得分，在参数矩阵之间自适应地分配参数预算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b361a6fa-6bf9-473f-bd2a-ac76ce75da5d",
   "metadata": {},
   "source": [
    "## 十二、**实战** \n",
    "\n",
    "基于ChatGLM2或Llama2， 微调一个同时具有NLU和问答能力对话机器人\n",
    "\n",
    "### 12.1、数据源\n",
    "\n",
    "酒店预订场景\n",
    "https://github.com/thu-coai/CrossWOZ\n",
    "\n",
    "酒店数据库\n",
    "https://github.com/thu-coai/CrossWOZ/blob/master/data/crosswoz/database/hotel_db.json\n",
    "\n",
    "### 12.2、数据增强\n",
    "\n",
    "- 从CrossWOZ数据集中抽取了只关于酒店的对话\n",
    "- 利用ChatGPT进行如下修改和补充\n",
    "    - 对设施的描述更口语化：“找一家有国际长途电话的酒店” -> “找一家能打国际长途的酒店”\n",
    "    - 补充一定比例的多轮问答，和结束语对话（p=0.3）\n",
    "    - 补充按酒店名（简称）、价格上限查询的对话（原数据中没有这类说法）\n",
    " \n",
    "最终按8:1:1拆分训练集、验证集和测试集\n",
    "\n",
    "**样本举例**\n",
    "\n",
    "```json\n",
    "[\r\n",
    "    {\r\n",
    "        \"role\": \"user\",\r\n",
    "        \"content\": \"请帮我找一个评分是4.5分以上的酒店。\"\r\n",
    "    },\r\n",
    "    {\r\n",
    "        \"role\": \"search\",\r\n",
    "        \"arguments\": {\r\n",
    "            \"rating_range_lower\": 4.5\r\n",
    "        }\r\n",
    "    },\r\n",
    "    {\r\n",
    "        \"role\": \"return\",\r\n",
    "        \"records\": [\r\n",
    "            {\r\n",
    "                \"name\": \"北京天伦王朝酒店\",\r\n",
    "                \"type\": \"豪华型\",\r\n",
    "                \"address\": \"北京东城区王府井大街50号\",\r\n",
    "                \"subway\": \"灯市口地铁站A口\",\r\n",
    "                \"phone\": \"010-58168888\",\r\n",
    "                \"facilities\": [\r\n",
    "                    \"公共区域和部分房间提供wifi\",\r\n",
    "                    \"宽带上网\",\r\n",
    "                    \"国际长途电话\",\r\n",
    "                    \"吹风机\",\r\n",
    "                    \"24小时热水\",\r\n",
    "                    \"西式餐厅\",\r\n",
    "                    \"中式餐厅\",\r\n",
    "                    \"残疾人设施\",\r\n",
    "                    \"室内游泳池\",\r\n",
    "                    \"会议室\",\r\n",
    "                    \"健身房\",\r\n",
    "                    \"无烟房\",\r\n",
    "                    \"商务中心\",\r\n",
    "                    \"桑拿\",\r\n",
    "                    \"棋牌室\",\r\n",
    "                    \"早餐服务\",\r\n",
    "                    \"接站服务\",\r\n",
    "                    \"接机服务\",\r\n",
    "                    \"接待外宾\",\r\n",
    "                    \"洗衣服务\",\r\n",
    "                    \"行李寄存\",\r\n",
    "                    \"租车\",\r\n",
    "                    \"叫醒服务\"\r\n",
    "                ],\r\n",
    "                \"price\": 815.0,\r\n",
    "                \"rating\": 4.5,\r\n",
    "                \"hotel_id\": 4\r\n",
    "            }\r\n",
    "        ]\r\n",
    "    },\r\n",
    "    {\r\n",
    "        \"role\": \"assistant\",\r\n",
    "        \"content\": \"推荐您去北京天伦王朝酒店。\"\r\n",
    "    },\r\n",
    "    {\r\n",
    "        \"role\": \"user\",\r\n",
    "        \"content\": \"这家酒店的价格是多少？\"\r\n",
    "    },\r\n",
    "    {\r\n",
    "        \"role\": \"assistant\",\r\n",
    "        \"content\": \"北京天伦王朝酒店的价格是每晚815元。\"\r\n",
    "    },\r\n",
    "    {\r\n",
    "        \"role\": \"user\",\r\n",
    "        \"content\": \"好的，我决定入住北京天伦王朝酒店了。\"\r\n",
    "    },\r\n",
    "    {\r\n",
    "        \"role\": \"assistant\",\r\n",
    "        \"content\": \"好的，祝您入住愉快！\"\r\n",
    "    }\r\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5114ca7-f6fd-4007-b186-bd49cad82850",
   "metadata": {},
   "source": [
    "### 12.2、ChatGLM2 的数据加载方式分析\n",
    "\n",
    "输入本质上就是一段按格式拼接的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ab5ff6-764c-49bf-830a-0ba0d8479d21",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3693428564.py, line 70)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 70\u001b[0;36m\u001b[0m\n\u001b[0;31m    return model_inputs`\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/THUDM/chatglm2-6b/blob/main/tokenization_chatglm.py\n",
    "# history = [(\"你好\",\"你好，我是AI\"),(\"你多大\",\"我有6B参数\")]\n",
    "def build_prompt(self, query, history=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    prompt = \"\"\n",
    "    for i, (old_query, response) in enumerate(history):\n",
    "        prompt += \"[Round {}]\\n\\n问：{}\\n\\n答：{}\\n\\n\".format(i + 1, old_query, response)\n",
    "    prompt += \"[Round {}]\\n\\n问：{}\\n\\n答：\".format(len(history) + 1, query)\n",
    "    return prompt\n",
    "\n",
    "    \n",
    "# https://github.com/THUDM/ChatGLM2-6B/blob/main/ptuning/main.py\n",
    "def preprocess_function_eval(examples):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[prompt_column])):\n",
    "        if examples[prompt_column][i] and examples[response_column][i]:\n",
    "            query = examples[prompt_column][i]\n",
    "            history = examples[history_column][i] if history_column is not None else None\n",
    "            prompt = tokenizer.build_prompt(query, history)\n",
    "            inputs.append(prompt)\n",
    "            targets.append(examples[response_column][i])\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, truncation=True, padding=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    if data_args.ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function_train(examples):\n",
    "    max_seq_length = data_args.max_source_length + data_args.max_target_length + 1\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "    for i in range(len(examples[prompt_column])):\n",
    "        if examples[prompt_column][i] and examples[response_column][i]:\n",
    "            query, answer = examples[prompt_column][i], examples[response_column][i]\n",
    "\n",
    "            history = examples[history_column][i] if history_column is not None else None\n",
    "            prompt = tokenizer.build_prompt(query, history)\n",
    "\n",
    "            prompt = prefix + prompt\n",
    "            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n",
    "                                     max_length=data_args.max_source_length)\n",
    "\n",
    "            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n",
    "                                     max_length=data_args.max_target_length)\n",
    "\n",
    "            context_length = len(a_ids)\n",
    "            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n",
    "            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n",
    "            \n",
    "            pad_len = max_seq_length - len(input_ids)\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "            labels = labels + [tokenizer.pad_token_id] * pad_len\n",
    "            if data_args.ignore_pad_token_for_loss:\n",
    "                labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
    "\n",
    "            model_inputs[\"input_ids\"].append(input_ids)\n",
    "            model_inputs[\"labels\"].append(labels)\n",
    "\n",
    "    return model_inputs`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadb9dc-982d-4183-b691-40946115511d",
   "metadata": {},
   "source": [
    "### 12.2.1、**知识点 1：**数据是怎么拼接的\n",
    "\n",
    "<img src=\"batch.png\" width=400px/>\n",
    "\n",
    "### 12.2.2、**知识点 2：**为什么训练时和测试时数据拼接方式不同\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b> Batch 拼接时 Padding 在哪边\n",
    "</div>\n",
    "\n",
    "<img src=\"training-batch.png\" width=400px/>\n",
    "\n",
    "<img src=\"inference-batch.png\" width=400px/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc8009-8975-4a4b-a650-66c76dca191e",
   "metadata": {},
   "source": [
    "### 12.3、结果的评价方式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3f6a3-113d-4279-898d-733e69494f67",
   "metadata": {},
   "source": [
    "```python\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if data_args.ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    score_dict = {\n",
    "        \"rouge-1\": [],\n",
    "        \"rouge-2\": [],\n",
    "        \"rouge-l\": [],\n",
    "        \"bleu-4\": []\n",
    "    }\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        hypothesis = list(jieba.cut(pred))\n",
    "        reference = list(jieba.cut(label))\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(' '.join(hypothesis), ' '.join(reference))\n",
    "        result = scores[0]\n",
    "\n",
    "        for k, v in result.items():\n",
    "            score_dict[k].append(round(v[\"f\"] * 100, 4))\n",
    "        bleu_score = sentence_bleu([list(label)], list(\n",
    "            pred), smoothing_function=SmoothingFunction().method3)\n",
    "        score_dict[\"bleu-4\"].append(round(bleu_score * 100, 4))\n",
    "\n",
    "    for k, v in score_dict.items():\n",
    "        score_dict[k] = float(np.mean(v))\n",
    "    return score_dict\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb2bd2-321a-4d3b-9a73-cf242151ac84",
   "metadata": {},
   "source": [
    "### 12.3.1、**知识点 3：**为什么不直接看 Loss\n",
    "\n",
    "语言模型在训练过程中 Loss 是如何计算的\n",
    "\n",
    "<img src=\"lm_loss.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "考虑下面的情况：\n",
    "\n",
    "<img src=\"lm_loss2.png\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "相似度高，但是 loss 大，所以 loss 没法准确反应出我们真实的预期\n",
    "\n",
    "### 12.3.2、**知识点 4：**生成模型常用的评价方法\n",
    "\n",
    "- BLEU Score:\n",
    "  - 计算输出与参照句之间的 n-gram 准确率（n=1...4）\n",
    "  - 对短输出做惩罚\n",
    "  - 在整个测试集上平均下述值\n",
    "\n",
    "$\\mathrm{BLEU}_4=\\min\\left(1,\\frac{output-length}{reference-length}\\right)\\left(\\prod_{i=1}^4 precision_i\\right)^{\\frac{1}{4}}$\n",
    "\n",
    "- Rouge Score:\n",
    "\n",
    "  - Rouge-N：将模型生成的结果和标准结果按 N-gram 拆分后，只计算召回率；\n",
    "  - Rouge-L: 利用了最长公共子序列（Longest Common Sequence），计算：$P=\\frac{LCS(c,r)}{len(c)}$, $R=\\frac{LCS(c,r)}{len(r)}$, $F=\\frac{(1+\\beta^2)PR}{R+\\beta^2P}$\n",
    "\n",
    "- 对比 BLEU 与 ROUGE\n",
    "  - BLEU 能评估流畅度，但指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强）\n",
    "  - ROUGE 不管流畅度，所以只适合深度学习的生成模型：结果都是流畅的前提下，ROUGE 反应参照句中多少内容被生成的句子包含（召回）\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b> 为什么不能直接用BLEU或者ROUGE作为Loss函数？\n",
    "</div>\n",
    " \n",
    "### 12.3.3、编写训练代码\n",
    "\n",
    "见附件\n",
    "\n",
    "### 12.3.4、测试集效果\n",
    "\n",
    "| Model       | Method      | BLEU-4 | SLOT-P | SLOT-R | SLOT-F1 | \n",
    "| :---------- | :---------- | :----: | :-----: | :-----: | :-----: | \n",
    "| ChatGLM2-6B  | P-Tuning V2 | 61.44 |  93.68 |  90.51  |  92.07  |   \n",
    "|             | LoRA        | **63.45**  |  94.32  |  93.41  |  93.86  |   \n",
    "| Llama2-7B | QLoRA | 63.05 | **95.96** |  **95.50**  |  **95.73**  | \n",
    "\n",
    "### 12.3.5、训练过程\n",
    "\n",
    "**ChatGLM 2 (6B) + P-Tuning V2**\n",
    "\n",
    "<img src=\"log-chatglm2-pt2.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "**ChatGLM 2 (6B) + LoRA**\n",
    "\n",
    "<img src=\"log-chatglm2-lora.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "**Llama 2 (7B) + QLoRA**\n",
    "\n",
    "<img src=\"log-llama2-qlora.png\" style=\"margin-left: 0px\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e1297-0e8e-43db-a259-d2e257e4e168",
   "metadata": {},
   "source": [
    "## 十五、即想要开放能力又想要垂直能力怎么办\n",
    "\n",
    "**大模型的训练过程（宏观）**\n",
    "\n",
    "<img src=\"train_llm.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "**混合通用领域和垂直领域数据（举例：度小满--轩辕）**\n",
    "\n",
    "<img src=\"xuanyuan.png\" style=\"margin-left: 0px\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8772d-ddcb-419b-96bd-463a2e6c2dd7",
   "metadata": {},
   "source": [
    "## 十六、数据准备与处理\n",
    "\n",
    "### 16.1、数据采集\n",
    "\n",
    "- 自然来源（如业务日志）：真实数据\n",
    "- Web 抓取：近似数据\n",
    "- 人造\n",
    "\n",
    "### 16.2、数据标注\n",
    "\n",
    "- 专业标注公司\n",
    "  - 定标准，定验收指标\n",
    "  - 预标注\n",
    "  - 反馈与优化\n",
    "  - 正式标注\n",
    "  - 抽样检查：合格->验收；不合格->返工\n",
    "- 众包\n",
    "  - 定标准，定检验指标\n",
    "  - 抽样每个工作者的质量\n",
    "  - 维系高质量标注者社区\n",
    "- 主动学习：通过模型选择重要样本，由专家标注，再训练模型\n",
    "- 设计产品形态，在用户自然交互中产生标注数据（例如点赞、收藏）\n",
    "\n",
    "### 16.3、数据清洗\n",
    "\n",
    "- 去除不相关数据\n",
    "- 去除冗余数据（例如重复的样本）\n",
    "- 去除误导性数据（业务相关）\n",
    "\n",
    "### 16.4、样本均衡性\n",
    "\n",
    "- 尽量保证每个标签（场景/子问题）都有足够多的训练样本\n",
    "- 每个标签对应的数据量尽量相当\n",
    "  - 或者在保证每个标签样本充值的前提下，数据分布尽量接近真实业务场景的数据分布\n",
    "- 数据不均衡时的策略\n",
    "  - 数据增强：为数据不够类别造数据：（1）人工造；（2）通过模板生成再人工标注；（3）由模型自动生成（再人工标注/筛选）\n",
    "  - 数据少的类别数据绝对数量也充足时，Downsample 一般比 Upsample 效果好\n",
    "  - 实在没办法的话，在训练 loss 里加权（一般不是最有效的办法）\n",
    "- 根据业务属性，保证其他关键要素的数据覆盖，例如：时间因素、地域因素、用户年龄段等\n",
    "\n",
    "### 16.5、数据集构建\n",
    "\n",
    "- 数据充分的情况下\n",
    "  - 切分训练集（训练模型）、验证集（验证超参）、测试集（检验最终模型+超参的效果）\n",
    "  - 以随机采样的方式保证三个集合的数据分布一致性\n",
    "  - 在以上三个集合里都尽量保证各个类别/场景的数据覆盖\n",
    "- 数据实在太少\n",
    "  - 交叉验证\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa23923-43e3-405f-95a6-d8d2a11934c6",
   "metadata": {},
   "source": [
    "## 作业：训练自己的模型\n",
    "\n",
    "请跟随[实验指导](../lab/index.ipynb)完成本次作业。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
